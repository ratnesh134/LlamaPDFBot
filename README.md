# ü¶ô LlamaPDFBot - LLAMA 3.1

This is a **Streamlit-based PDF chatbot** that enables users to upload a **PDF document** and interact with it using **LLAMA 3.1-70B** via **Groq**. It uses **FAISS vector storage** and **Hugging Face embeddings** for efficient document retrieval and chat memory.

## Key Highlights:
‚úÖ Developed an AI-powered chatbot using LLAMA 3.1-70B via Groq for document-based question-answering.
‚úÖ Built a PDF processing pipeline that extracts, splits, and embeds text using Hugging Face embeddings and stores it in FAISS for efficient retrieval.
‚úÖ Designed and implemented a Retrieval-Augmented Generation (RAG) system, ensuring accurate and context-aware responses.
‚úÖ Integrated Streamlit for an interactive UI, enabling users to upload PDFs, ask questions, and receive responses dynamically.
‚úÖ Implemented Conversational Memory using LangChain‚Äôs ConversationBufferMemory, maintaining chat history for a seamless experience.
‚úÖ Utilized LangChain for efficient document chunking, embedding creation, and retrieval optimization.
‚úÖ Optimized model performance by fine-tuning retrieval settings and reducing response latency using efficient text-splitting strategies.
‚úÖ Implemented secure API handling using .env files to manage Groq API keys and ensure data protection.
‚úÖ Followed Agile development methodologies for iterative improvements and deployed locally with Python virtual environments for dependency management.

## Features
- üìÑ **PDF Upload & Processing**: Extracts text from uploaded PDFs.
- üß† **Vector Embeddings**: Uses **FAISS** and **Hugging Face** for document storage.
- ü§ñ **Conversational AI**: Implements **LLAMA 3.1-70B** via **Groq**.
- üîç **Retrieval-Augmented Generation (RAG)**: Provides responses based on document content.
- üí¨ **Chat Memory**: Stores conversation history.

## Installation
### 1Ô∏è‚É£ Clone the Repository
git clone https://github.com/ratnesh134/LlamaPDFBot.git
cd chat-with-doc-llama

### Setting up the Virtual Environment
python -m venv venv
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate  # Windows

### Installing the dependencies
pip install -r requirements.txt

### Set Up API Keys
Create a .env file in the project root and add your API keys:

GROQ_API_KEY=your_groq_api_key

### Run the Application
streamlit run app.py

## Usage

Upload a PDF document.

Ask questions related to the document.

The chatbot retrieves relevant content and provides context-aware responses.

Conversation history is maintained for continuity.

## Technologies Used
Streamlit for UI

LangChain for LLM interactions

FAISS for vector search

Hugging Face Embeddings for document representation

Groq for LLAMA 3.1-70B

# Here's a flow diagram representing the workflow of your Chat with Doc - LLAMA 3.1 project.

## Flow Breakdown:
User Uploads PDF ‚Üí The PDF document is loaded and processed.

Text Extraction & Splitting ‚Üí The document is converted into text chunks.

Vector Embedding ‚Üí Text chunks are embedded using Hugging Face Embeddings and stored in FAISS.

User Asks a Question ‚Üí The query is processed and retrieved from the vector store.

LLAMA 3.1 Model Processing ‚Üí The Groq LLM answers based on retrieved content.

Response Displayed ‚Üí The answer and relevant document context are shown in Streamlit.
